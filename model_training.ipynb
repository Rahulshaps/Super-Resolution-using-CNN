{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OmdvOInTRjU3",
        "outputId": "f4307514-bedf-4f82-9587-43773e77d172"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nName: Aayush Sanghvi, Rahul Sha\\nClass: CS 7180 Advanced Perception\\nDate: 17th September 2025\\nPurpose: Implements a Super-Resolution Convolutional Neural Network (SRCNN) to enhance image resolution.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "Name: Aayush Sanghvi, Rahul Sha\n",
        "Class: CS 7180 Advanced Perception\n",
        "Date: 17th September 2025\n",
        "Purpose: Implements a Super-Resolution Convolutional Neural Network (SRCNN) to enhance image resolution.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f34c1215",
        "outputId": "8427faa1-d8ba-4e9e-de17-f0c7ecdcef16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.75.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.9)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUUsxHMVYHNa",
        "outputId": "1f7af26b-aff0-4c2f-f81c-2610bf594700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import torchvision, torchvision.transforms as transforms\n",
        "import torch, torch.nn as nn\n",
        "import random, subprocess, os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# To read and write to google drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sz6noG-2YOaG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def dataloader: Loads the dataloader of the STL-10 Dataset, downscaled by a factor of 'scale' to\n",
        "                generate low resolution images and the original images as high resolution images.\n",
        "\n",
        "Arguments:\n",
        "    crop_size: image size of the square sub images the model has been trained on.\n",
        "    batch_size: batch size of the dataloader\n",
        "    num_workers: number of workers to use for the dataloader\n",
        "    scale: Scale by which the low resolution image is downscaled\n",
        "\n",
        "Output:\n",
        "    low and high image datasets: dataloader iterable to be able to train on the images\n",
        "\"\"\"\n",
        "def stl_dataloader(crop_size: int = 33, batch_size: int = 128, num_workers: int = 1, scale: float = 2.0):\n",
        "\n",
        "    # Write transforms for TenCrop and for generating low res images using bicubic interpolation (interpolation = 3)\n",
        "    transform_high_res = transforms.Compose([\n",
        "            transforms.TenCrop(crop_size),\n",
        "            transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))\n",
        "        ])\n",
        "    transform_low_res = transforms.Compose([\n",
        "            transforms.Resize(int(96 / scale), interpolation=3),\n",
        "            transforms.Resize(96, interpolation=3),\n",
        "            transform_high_res\n",
        "        ])\n",
        "\n",
        "    # Make STL-10 dataset object\n",
        "    dataset_high_res = torchvision.datasets.STL10('.', transform = transform_high_res, download = True)\n",
        "    dataset_low_res = torchvision.datasets.STL10('.', transform = transform_low_res, download = False)\n",
        "\n",
        "    # Create the dataloader object using the transforms (Not shuffled since we will be checking progress on the same examples)\n",
        "    dataloader_high_res = torch.utils.data.DataLoader(dataset_high_res, batch_size = batch_size, num_workers = num_workers, shuffle = False)\n",
        "    dataloader_low_res = torch.utils.data.DataLoader(dataset_low_res, batch_size = batch_size, num_workers = num_workers, shuffle = False)\n",
        "    return dataloader_low_res, dataloader_high_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KclT2ea_ZMKh"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "class SuperResolution: Defined the Super-Resolution CNN model.\n",
        "\n",
        "Architecture Overview:\n",
        "  1. layer_1 : Learns 128 different 9x9x3 filters to extract low-level features followed by ReLU for non-linear activation.\n",
        "  2. layer_2 : Maps the 128-channel feature maps into a 64-channel layer followed by ReLU for non-linear feature transformation.\n",
        "  3. layer_3 : Reconstructs the final high-resolution RGB image from the feature representation.\n",
        "\n",
        "Output:\n",
        "    x : Super-resolved RGB image (B, 3, H, W)\n",
        "    y : Intermediate 64-channel feature map for visualization or auxiliary loss\n",
        "\n",
        "\"\"\"\n",
        "class SuperResolution(nn.Module):\n",
        "    def __init__(self, sub_image: int = 33, spatial: list = [9, 5, 5], filter: list = [128, 64], num_channels: int = 3):\n",
        "        super().__init__()\n",
        "        self.layer_1 = nn.Conv2d(num_channels, filter[0], spatial[0], padding = spatial[0] // 2)\n",
        "        self.layer_2 = nn.Conv2d(filter[0], filter[1], spatial[1], padding = spatial[1] // 2)\n",
        "        self.layer_3 = nn.Conv2d(filter[1], num_channels, spatial[2], padding = spatial[2] // 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, image_batch):\n",
        "        x = self.layer_1(image_batch)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_2(x)\n",
        "        y = self.relu(x)\n",
        "        x = self.layer_3(y)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FwLvdlCTZPmX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def train:  This script defines and runs the full training loop for\n",
        "            the SRCNN super-resolution model.It loads paired low- and\n",
        "            high-resolution image crops from the STL dataset. Model is fed\n",
        "            low-resolution batches through the three-layer SuperResolution\n",
        "            network to produce high-resolution reconstructions and\n",
        "            intermediate feature maps.\n",
        "\n",
        "Output: Model trained for 500 epochs with MSE loss optimized using Adam optimizer.\n",
        "\"\"\"\n",
        "def train():\n",
        "\n",
        "    # Initialize model, data, writer, optimizer, and backward count\n",
        "    low_res_loader, high_res_loader = stl_dataloader()\n",
        "    model = SuperResolution()\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), 1e-04)\n",
        "    writer = SummaryWriter()\n",
        "    n = 0\n",
        "\n",
        "    for epoch in tqdm(range(500), desc= \"Training\", ncols = 120):\n",
        "        for low_res, high_res in zip(low_res_loader, high_res_loader):\n",
        "\n",
        "            # Convert TenCrop tuple into a trainable shape of (batch_size * 10, c, h, w)\n",
        "            low_res_batch, high_res_batch = low_res[0], high_res[0]\n",
        "            _, _, c, h, w = low_res_batch.size()\n",
        "            low_res_batch, high_res_batch = low_res_batch.to(device), high_res_batch.to(device)\n",
        "            low_res_batch, high_res_batch = low_res_batch.view(-1, c, h, w), high_res_batch.view(-1, c, h, w)\n",
        "            reconstructed_batch, intermediate = model(low_res_batch)\n",
        "\n",
        "            # Calculate gradients and make a backward step on MSE loss\n",
        "            loss_fn = nn.MSELoss()\n",
        "            loss = loss_fn(high_res_batch, reconstructed_batch)\n",
        "            loss_to_compare = loss_fn(high_res_batch, low_res_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Clamp the image between 0 and 1 and prepare transforms and image arrays to write on tensorboard\n",
        "            to_pil = torchvision.transforms.ToPILImage()\n",
        "            resize = torchvision.transforms.Resize((48 * 7, 144 * 7))\n",
        "            other_resize = torchvision.transforms.Resize((48 * 5, 48 * 5))\n",
        "            to_tensor = torchvision.transforms.ToTensor()\n",
        "            ind = 4\n",
        "            image = to_pil(torch.cat((low_res_batch[ind], high_res_batch[ind], reconstructed_batch[ind]), dim = 2).cpu())\n",
        "            image = to_tensor(resize(image))\n",
        "            image = image.clamp(0, 1)\n",
        "            n += 1\n",
        "            psnr = 10 * torch.log10(1 / loss)\n",
        "            psnr_tc = 10 * torch.log10(1 / loss_to_compare)\n",
        "\n",
        "            # Write relevant scalars and comparitive images on tensorboard\n",
        "            writer.add_scalar(\"MSE loss\", loss * (255 ** 2), n)\n",
        "            writer.add_scalar(\"PSNR of Reconstruction\", psnr, n)\n",
        "            writer.add_scalar(\"PSNR of BiCubic Interpolation (For comparision)\", psnr_tc, n)\n",
        "            writer.add_image(\"Low Resolution Image | High Resolution Image | Reconstructed Image\", image, n, dataformats='CHW')\n",
        "\n",
        "\n",
        "            index = 40 #Chooses image patch to visualize on, up till 80 (Size of the remnant batch)\n",
        "            channels_to_visualize = [1, 2, 3, 4, 5, 6, 7, 8]  #Channel numbers out of 64 to visualize\n",
        "\n",
        "            # Write the intermediate layer visualizations and also write to drive, to download and create animated gifs\n",
        "            patch = to_tensor(other_resize(to_pil(high_res_batch.detach().cpu()[index])))\n",
        "            writer.add_image(\"Image Patch {}\".format(index), patch, n, dataformats='CHW')\n",
        "            pil_patch = to_pil(patch)\n",
        "            pil_patch.save('/content/drive/My Drive/isr/patch_{}.png'.format(index))\n",
        "\n",
        "            # Write the progress of training on two standard examples - 25 and 30 0f last batch\n",
        "            os.makedirs('/content/drive/My Drive/isr/r1', exist_ok=True)\n",
        "            r1 = to_tensor(other_resize(to_pil(reconstructed_batch.detach().cpu()[25])))\n",
        "            pil_r1 = to_pil(r1)\n",
        "            pil_r1.save('/content/drive/My Drive/isr/r1/frame_{}.png'.format(epoch))\n",
        "\n",
        "            os.makedirs('/content/drive/My Drive/isr/r2', exist_ok=True)\n",
        "            r2 = to_tensor(other_resize(to_pil(reconstructed_batch.detach().cpu()[30])))\n",
        "            pil_r2 = to_pil(r2)\n",
        "            pil_r2.save('/content/drive/My Drive/isr/r2/frame_{}.png'.format(epoch))\n",
        "\n",
        "            for feature in channels_to_visualize:\n",
        "                os.makedirs('/content/drive/My Drive/isr/channel_{}'.format(feature), exist_ok=True)\n",
        "                visualization = to_tensor(other_resize(to_pil(intermediate.detach().cpu()[index, feature,:,:])))\n",
        "                writer.add_image(\"Channel {}\".format(feature), visualization, n, dataformats='CHW')\n",
        "                pil_vis = to_pil(visualization)\n",
        "                pil_vis.save('/content/drive/My Drive/isr/channel_{}/frame_{}.png'.format(feature, epoch))\n",
        "\n",
        "        # Save the model for every epoch\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/isr/isr_best_2.pth'.format(n))\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train()"
      ],
      "metadata": {
        "id": "WxYWq9BFSE6Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}